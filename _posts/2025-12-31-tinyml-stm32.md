---
layout:     post
title:      "保姆级教程：手把手教你把 AI 模型部署到 STM32H750 (TinyML)"
subtitle:   "从 Python 训练到 CubeMX 部署，全流程实战指南"
date:       2025-12-31 10:00:00
author:     "Pigeon"
header-img: "img/post-bg-2015.jpg"
tags:
    - TinyML
    - STM32
    - AI
    - Python
---

> 随着物联网的发展，在单片机上运行 AI 模型（TinyML）变得越来越流行。本文将以 **STM32H750XBH6** 为例，手把手教你如何从零开始，训练一个简单的神经网络，并将其部署到单片机上运行。

## 0. 准备工作

在开始之前，你需要准备以下工具：

1.  **硬件**：STM32H750XBH6 开发板（或其他 STM32 开发板，H7 系列性能强劲，适合跑大模型，但本教程的小模型 F1/F4 也能跑）。
2.  **软件**：
    *   **Python 环境** (建议安装 Anaconda 或 Miniconda)。
    *   **TensorFlow** (`pip install tensorflow`)。
    *   **STM32CubeMX** (务必更新到最新版)。
    *   **STM32CubeIDE** (或 Keil/IAR)。
    *   **X-CUBE-AI 包** (在 CubeMX 中下载)。

---

## 1. 第一步：在 Python 中训练模型

为了演示方便，我们训练一个简单的**正弦波预测模型**。输入一个 $x$ 值，模型预测 $\sin(x)$。

新建一个 `train.py` 文件，复制以下代码并运行：

```python
import tensorflow as tf
import numpy as np
import math

# 1. 生成数据集
SAMPLES = 1000
x_values = np.random.uniform(low=0, high=2*math.pi, size=SAMPLES)
np.random.shuffle(x_values)
y_values = np.sin(x_values)

# 添加一点噪声，模拟真实数据
y_values += 0.1 * np.random.randn(*y_values.shape)

# 2. 搭建模型 (简单的全连接网络)
model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(16, activation='relu', input_shape=(1,)))
model.add(tf.keras.layers.Dense(16, activation='relu'))
model.add(tf.keras.layers.Dense(1)) # 输出层

# 3. 编译与训练
model.compile(optimizer='adam', loss='mse', metrics=['mae'])
model.fit(x_values, y_values, epochs=200, batch_size=32, validation_split=0.2)

# 4. 保存模型 (这是关键！)
model.save('sine_model.h5')
print("模型已保存为 sine_model.h5")
```

运行结束后，你会得到一个 `sine_model.h5` 文件。这就是我们要烧录进单片机的“大脑”。

---

## 2. 第二步：STM32CubeMX 配置

这是最关键的一步，我们将使用 ST 官方的 **X-CUBE-AI** 插件将 `.h5` 文件转换为 C 代码。

### 2.1 基础配置
1.  打开 **STM32CubeMX**，选择 MCU `STM32H750XBH6`。
2.  配置 **System Core** -> **SYS** -> **Debug** (Serial Wire)。
3.  配置 **Connectivity** -> **USART1** (用于串口打印结果)。
4.  配置 **Clock Configuration**，将 H7 的主频拉满 (400MHz+)。

### 2.2 启用 X-CUBE-AI
1.  点击顶部菜单的 **Software Packs** -> **Select Components**。
2.  找到 **STMicroelectronics.X-CUBE-AI**，展开并勾选 **Core**，版本选择最新。
3.  点击 OK，回到主界面，左侧会出现 **Software Packs** 栏目。
4.  点击 **STMicroelectronics.X-CUBE-AI**，勾选 **Artificial Intelligence X-CUBE-AI**。

### 2.3 导入模型
1.  在 X-CUBE-AI 的配置页面中，点击 **Add network**。
2.  **Name** 设为 `sine_model` (记住这个名字)。
3.  **Model inputs** 选择 **Keras**。
4.  **Model** 浏览选择刚才生成的 `sine_model.h5`。
5.  点击 **Analyze** 按钮。
    *   CubeMX 会分析模型的 RAM 和 Flash 占用。
    *   对于这个小模型，Flash 占用极小（几 KB），RAM 也很小。
    *   如果显示绿色图标，说明模型可以在该 MCU 上运行。

### 2.4 生成代码
1.  切换到 **Project Manager** 标签。
2.  **Toolchain/IDE** 选择 **STM32CubeIDE**。
3.  点击 **Generate Code**。

---

## 3. 第三步：编写 C 代码

打开生成的工程，我们需要在 `main.c` 中调用 AI 库。

### 3.1 包含头文件
在 `main.c` 顶部添加：

```c
/* Private includes ----------------------------------------------------------*/
/* USER CODE BEGIN Includes */
#include "sine_model.h"
#include "sine_model_data.h"
#include <stdio.h> // 用于 printf
/* USER CODE END Includes */
```

### 3.2 定义变量
在 `main` 函数之前定义 AI 相关的句柄和 buffer：

```c
/* USER CODE BEGIN PV */
static ai_handle network = AI_HANDLE_NULL;
AI_ALIGNED(32) static ai_u8 activations[AI_SINE_MODEL_DATA_ACTIVATIONS_SIZE];
AI_ALIGNED(32) static ai_float in_data[AI_SINE_MODEL_IN_1_SIZE];
AI_ALIGNED(32) static ai_float out_data[AI_SINE_MODEL_OUT_1_SIZE];

// 输入输出 buffer 结构体
static ai_buffer *ai_input;
static ai_buffer *ai_output;
/* USER CODE END PV */
```

### 3.3 初始化 AI 模型
在 `main` 函数中，`while(1)` 之前：

```c
  /* USER CODE BEGIN 2 */
  ai_error err;
  
  // 1. 创建网络实例
  err = ai_sine_model_create(&network, AI_SINE_MODEL_DATA_CONFIG);
  if (err.type != AI_ERROR_NONE) {
      printf("AI Create Error\r\n");
      while(1);
  }

  // 2. 初始化网络 (绑定 activation buffer)
  ai_network_params params = {
      .params = ai_sine_model_data_weights_get(),
      .activations = activations
  };
  
  if (!ai_sine_model_init(network, &params)) {
      printf("AI Init Error\r\n");
      while(1);
  }

  // 3. 获取输入输出 buffer 指针
  ai_input = ai_sine_model_inputs_get(network, NULL);
  ai_output = ai_sine_model_outputs_get(network, NULL);
  /* USER CODE END 2 */
```

### 3.4 运行推理 (Inference)
在 `while(1)` 循环中，我们模拟输入数据并运行模型：

```c
  /* USER CODE BEGIN WHILE */
  float test_x = 0.0f;
  
  while (1)
  {
    // 1. 准备输入数据 (0 到 2pi 循环)
    test_x += 0.1f;
    if (test_x > 6.28f) test_x = 0.0f;
    
    // 将数据填入 buffer (注意类型转换)
    ((ai_float *)ai_input[0].data)[0] = (ai_float)test_x;

    // 2. 运行推理
    ai_i32 batch = ai_sine_model_run(network, ai_input, ai_output);
    
    if (batch != 1) {
        printf("AI Run Error\r\n");
    } else {
        // 3. 获取结果
        float prediction = ((ai_float *)ai_output[0].data)[0];
        
        // 4. 打印结果 (格式: 输入, 预测值, 真实值)
        // 可以在 Serial Plotter 中观察波形
        printf("Input: %.2f, Pred: %.2f, Real: %.2f\r\n", 
               test_x, prediction, sin(test_x));
    }
    
    HAL_Delay(100);
    /* USER CODE END WHILE */
```

*(注意：为了使用 `printf`，你需要重定向 `fputc` 到 UART，这里不再赘述)*

---

## 4. 流程总结图

![TinyML Workflow](/img/post-tinyml/tinyml-workflow.svg)
*(图片来源: 自制流程图)*

## 5. 常见问题 (FAQ)

*   **Q: 编译报错 `RAM` 不足？**
    *   A: H750 内部 Flash 较小，但 RAM 很大。如果模型权重放在 Flash 放不下，可以配置链接脚本放到外部 Flash (QSPI)，或者在 CubeMX 中选择将权重放到 RAM (如果 RAM 够大)。对于本例的小模型，内部 Flash 足够。
*   **Q: `ai_create` 失败？**
    *   A: 检查 `activations` 数组是否按照 32 字节对齐 (`AI_ALIGNED(32)`)，这是神经网络加速器要求的。
*   **Q: 预测结果不准？**
    *   A: 检查训练时的归一化处理。如果训练时输入归一化到了 [0, 1]，单片机端输入前也要做同样的归一化。本例中我们直接用了原始值，所以不需要。

希望这篇教程能帮你迈出 TinyML 的第一步！
